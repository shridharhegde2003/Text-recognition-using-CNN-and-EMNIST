{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST Handwritten Character Recognition - Advanced Training\n",
    "\n",
    "This notebook trains a more robust Convolutional Neural Network (CNN) on the EMNIST `byclass` dataset with the following improvements:\n",
    "\n",
    "1.  **Increased Data**: Uses **50%** of the training dataset.\n",
    "2.  **Data Augmentation**: Applies real-time image augmentation (rotation, shift, shear, zoom) to make the model more robust to variations in handwriting.\n",
    "3.  **Deeper Model**: Uses a more complex CNN architecture with more filters and dropout layers to capture finer details.\n",
    "4.  **Interrupt-Safe Training**: Includes callbacks to save training history and the best model weights after each epoch, so progress is not lost if training is stopped.\n",
    "5.  **Comprehensive Evaluation**: Generates and saves plots for training history, a confusion matrix, and a detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Interrupt-Safe History Callback\n",
    "\n",
    "This custom callback saves the training history (loss, accuracy, etc.) to a file after every single epoch. If training is interrupted, we can still load this file and plot our progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to save history after each epoch.\"\"\"\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        # Save history to a pickle file after each epoch\n",
    "        with open('training_history.pkl', 'wb') as f:\n",
    "            pickle.dump(self.history, f)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "         print(\"Training finished. History is saved in 'training_history.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare EMNIST Dataset\n",
    "\n",
    "We load the `emnist/byclass` dataset, which contains 62 classes (digits, uppercase, and lowercase letters). We will use **50% of the training data** and 50% of the test data to speed up the process while improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 50% of the training data and 50% of the test data\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'emnist/byclass',\n",
    "    split=['train[:50%]', 'test[:50%]'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "num_classes = ds_info.features['label'].num_classes\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "# Define the human-readable labels\n",
    "labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "print(f'Total labels: {len(labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Batch the Data\n",
    "\n",
    "We normalize the pixel values to be between 0 and 1. The EMNIST dataset images are also transposed (rotated and flipped), so we fix that here. We also convert the datasets into numpy arrays for use with the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # EMNIST images are rotated and flipped, we need to transpose them back\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess).cache().prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Convert datasets to numpy arrays for ImageDataGenerator\n",
    "ds_train_images = np.array([x for x, y in ds_train])\n",
    "ds_train_labels = np.array([y for x, y in ds_train])\n",
    "ds_test_images = np.array([x for x, y in ds_test])\n",
    "ds_test_labels = np.array([y for x, y in ds_test])\n",
    "\n",
    "print(f'Training images shape: {ds_train_images.shape}')\n",
    "print(f'Test images shape: {ds_test_images.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n",
    "\n",
    "We use `ImageDataGenerator` to create modified versions of our training images on-the-fly. This helps the model generalize better to real-world handwriting that might be slightly rotated, shifted, or scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1, # randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.1,        # set range for random shear\n",
    "    zoom_range=0.1,         # set range for random zoom\n",
    "    horizontal_flip=False,  # EMNIST chars are not flipped\n",
    "    vertical_flip=False     # EMNIST chars are not flipped\n",
    ")\n",
    "\n",
    "# Fit the generator on our training data\n",
    "datagen.fit(ds_train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Improved CNN Model\n",
    "\n",
    "This model is deeper and wider than the previous version. It has more convolutional filters to learn more complex features and uses dropout to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Increased filters in the first convolutional layer\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # Increased filters in the second convolutional layer\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.3), # Added dropout for regularization\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # Increased density in the dense layer\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5), # Increased dropout\n",
    "    \n",
    "    # Output layer with softmax for multi-class classification\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "We train for 10 epochs. We use two callbacks:\n",
    "1.  `HistoryCallback`: Saves our metrics plot data.\n",
    "2.  `ModelCheckpoint`: Saves the best version of the model seen so far during training. This is our primary defense against losing work if the session is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_callback = HistoryCallback()\n",
    "\n",
    "# This callback will save the model with the best validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.h5',      # File to save the model\n",
    "    save_best_only=True,         # Only save a model if `val_accuracy` has improved\n",
    "    monitor='val_accuracy',      # Monitor validation accuracy\n",
    "    mode='max',                  # The higher the val_accuracy, the better\n",
    "    verbose=1                    # Print a message when the model is saved\n",
    ")\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    datagen.flow(ds_train_images, ds_train_labels, batch_size=256), # Use the data generator\n",
    "    epochs=10,\n",
    "    validation_data=(ds_test_images, ds_test_labels), # Use the original test set for validation\n",
    "    callbacks=[history_callback, checkpoint_callback]\n",
    ")\n",
    "print(\"--- Model training complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Training Results\n",
    "\n",
    "First, we load the training history from our pickle file. This ensures we can generate these plots even if the training was stopped and the `history` object in the notebook was lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the history from the pickle file for robust plotting\n",
    "with open('training_history.pkl', 'rb') as f:\n",
    "    history_data = pickle.load(f)\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "history_df = pd.DataFrame(history_data)\n",
    "history_df['epoch'] = history_df.index + 1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['epoch'], history_df['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history_df['epoch'], history_df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['epoch'], history_df['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history_df['epoch'], history_df['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plot_filename = 'training_history.png'\n",
    "plt.savefig(plot_filename)\n",
    "print(f\"Saved training history plot to '{plot_filename}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Best Model on the Test Set\n",
    "\n",
    "Now, we load the **best model** that was saved by our `ModelCheckpoint` callback. This ensures we are evaluating the version of the model that performed best on the validation data, not just the one from the final epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model saved during training\n",
    "print(\"Loading the best model from 'best_model.h5'...\")\n",
    "best_model = tf.keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(ds_test_images)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "report = classification_report(ds_test_labels, y_pred, target_names=labels)\n",
    "print(report)\n",
    "\n",
    "# Save the report to a text file\n",
    "report_filename = 'classification_report.txt'\n",
    "with open(report_filename, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"Saved classification report to '{report_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix gives us a detailed, visual breakdown of which classes the model is confusing with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(ds_test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# Save the confusion matrix plot to a file\n",
    "cm_filename = 'confusion_matrix.png'\n",
    "plt.savefig(cm_filename)\n",
    "print(f\"Saved confusion matrix to '{cm_filename}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Final Model for the Web App\n",
    "\n",
    "Finally, we save the best-performing model with the standard name `htr_model.h5` so it can be easily used by the Flask application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[8/8] Saving the final, best-performing model...\")\n",
    "best_model.save('htr_model.h5')\n",
    "print(\"--- Model training and evaluation complete. Best model saved as htr_model.h5 ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
